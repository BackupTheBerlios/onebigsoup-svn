
= 2005-06-22 =

== 2:44 PM ==

title: Version 1.3 Ideas:
* "STRING" record type - unambigous (no relative)
* extend "PATTERN" w/ $1, $2, $3, ...
* the return of names lists?

== 12:31 PM ==

Continue with links page, after "Service Names"

'''COMMIT LIST (output):'''
* all .png
* localnames.css
* servicenames.txt

'''COMMIT LIST (input):'''
* all .svg

'''DOWNLOAD LIST:'''
* servicenames.txt
* actual interface specifications
* .svg

= 2005-06-21 =

== 9:01 PM ==

title: Prioritizing, Recollecting Work To Do

* unprocessed data? ''straight'' into output directory

'''HIGH PRIORITY:'''

* (./) commit list for output directory
* (./) CSS, title, needs to look reasonable
* (./) visit every page, checking for errors
* (./) localnames.txt: NS "S" "servicenames.txt"
* HIGH tasks from page list
* utf-8 store localnames.txt
* update glossary language for pattern, namespace, extension data

PENDING INTERNET:
* actual interface specifications
* talk: LNNDS v1.2
* try connected linking, recheck pages

AFTER INTERNET RETRIEVALS:
* setup Local Names XML-RPC Store Interface page
* setup Local Names XML-RPC Query Interface page
* name LNXRStoreI && LNXRQueryI
* Fix Local Names Namespace Description Specification version 1.2 page!
* Update LNNDS language for namespaces, patterns, extension data

'''MEDIUM PRIORITY:'''

* interesting and explanatory image for front page
* index title look different
* region sets (#set HEAD)
* index: $HEAD broadcast localnames.txt, DOAP, RSS
** update RSS feed
** DOAP file
* auto-list used but undefined names
* MED tasks from page list
* check "about" images
* get SVG files
* check specification v1.2, about Local Names, 
* link menu to LNNDS v1.2, LNXRQueryI, LNXRStoreI
* separators between glossary entries

'''LOW PRIORITY:'''

* autoconvert SVG to PNG
* rewrite glossary entries
* work out "downloads" page, until we actually have it
* make sure separator=:
* LOW tasks from page list
* images in singhtext
* check News LNXRStoreI (post-Internet)
* check News LNXRQueryI (post-Internet)
* automatic link checking output
* standardize the language for namespace hopping, implicit traversal,
  explicit traversal, etc., etc., and then integrate with the glossary
* standardize canonical example records (LN, NS, PATTERN, and X)

'''Individual Pages:'''

specs: MED(rename title "Specifications")

''DONE!'' downloads: (./) HIGH(missing title)

about page: (./) HIGH(fix "What is Local Names?" link), MEDIUM(fix "[[spec]]"
link), MEDIUM(fix "the spec" link)

glossary page: LOW("Local Names server" - shouldn't require reading
interfaces)

news page: (./) HIGH(link "Local Names XML-RPC Store Interface", "Local Names
XML-RPC Query Interface"), (./) HIGH(missing title), LOW(link "LNXRStoreI"
"LNXRQueryI")


links page: (./) HIGH(link "Services Names"), MED(link "Local Names
Wrap"), MED(link "my.LocalNames" - standardize), MED("Web Browser
Redirect"), MED("jump to website"), MED("Local Names Wrap"),
MED("my.LocalNames"), (./) HIGH(missing title)

glossary: (./) HIGH(rename title) LOW(make S:WP:URN link point to exact
wikipedia entry for URN) MED(mention that "URL" might be a "relative
URL")

HIGH:
first pass over glossary entries to write:
* (./) URL
* (./) record type
* (./) LN
* (./) NS
* (./) PATTERN
* (./) X
* (./) namespace description
* (./) line
* (./) column
* (./) local name
* (./) name
* (./) namespace link
* (./) link key
* (./) pattern
* (./) pattern key
* (./) URL template
* (./) relative URL template (just join with URL template)
* relative URL
* extension data
* extension key
* extension values
* record

HIGH: second pass, check that everything is defined


== 8:28 PM ==

I left it in the middle of outputing localnames.txt;
Contine from the end of the Builder class.

== 10:01 AM ==

Work to do:

(nHL) means: "no Hard Link to ln.taoriver.net."
(nF) means: "not found"

ALL:
* need the actual text of the Local Names XML-RPC Store Interface
* need the actual text of the Local Names XML-RPC Query Interface
* need the actual text of LNNDS v1.1
* need the actual RSS feed
* need the DOAP

SINGHTEXT:
* region sets (#set HEAD)

WEBSITE.PY:
* (./) output localnames.txt that includes new names
* (./) "$set title: foo" handling
* read & write CSS
* read & write graphic images
** SVG autoconvert?

TEMPLATE:
* (./) "spec" to "specifications" (specs)
* (./) nHL (all links)
* (./) localnames.txt link needs to work

INDEX:
* (./) "Read more about Local Names" - nHL
* (./) table: nHL
* (./) "a Local Names description" - nHL
* $HEAD broadcast localnames.txt, DOAP, RSS, ...

LOCAL NAMES SPECIFICATIONS:
* need to relink "Local Names XML-RPC Store Interface"
* need to relink "Local Names XML-RPC Query Interface"
* ...to the actual specification itself.

LOCAL NAMES NAMESPACE DESCRIPTION SPECIFICATION v1.2
* nHL: understand the main ideas of Local Names
* LNNDS v1.1: need actual text, and link to it
* (./) Lion Kimbro: fix link
* lion@speakeasy.org: fix link (says name not found...)
* nHL: news
* nHL: RSS feed
* nf: Lion Kimbro's blog
* link to Lion Kimbro's blog RSS feed
* PAGE RENDERING STOPS! It's BROKEN!

NEWS:
* braodcast RSS

Okay, I think this is enough to be fixed right now...

...especially given that I can't get to the other pages.

But we can do this. {:)}= This is a good learning experience. {:)}=

Keep up the positive attitude! Remember Napoleon Hill; He's with us all
the way.


= 2005-06-13 =

== 8:30 AM ==

title: Work ''Still'' Remains

Bugfixes:
* remote XML-RPC lookups ''fails'' always

...and all the things from 2005-06-08, 9:03 AM.

How am I going to deal with name crunching?

The problem is where to place the logic for looking up names. So far,
I've just been doing name lookups by looking ''directly into a hash.''
Instead, I need to be ''performing some hash calculation, and then
looking into a hash.''

In fact, in greater details: I need to look through the dictionary
first for the literal text. ''Then'' check a hash version. ''Then''
proceed to visit neighors, one by one.

In essence, whatever the Local Names system would do, ''I'' need to do.

This implies, to me, however, ''going through Local Names core.''
Separate out code for resolving URLs, so I can attach a "null," or a
logically constructed names table, in addition to actually looking stuff
up on the Internet. Whatever code is used for resolving names on the
server, the same code is used to resolve names locally. It should even
be possible to delegate look-ups, in whole or in part, to another
server. Or to retrieve cached versions from a server.

In the meantime, I'm just going to use the straight names, as they
appear in the Local Names table, website title, or $name entry.

Once we have the query interface more or less settled out, and have a
working query implementation like we like, I may be reworking the part
of the website code that performs name-URL binding, to make use of it.


= 2005-06-08 =

== 9:03 AM ==

title: Still Work to Do

* (./) able to title a page one thing, but local name it another
** (./) ''just stick a ''$name'' at the top?''
** (./) '''No:''' Just use $set.
* sloppy name forming: automatically crunching a name up?
** "Local Names" becomes localnames
** two motivations:
*** friendly <a name> lines
*** local name linking


== 8:46 AM ==

Well, it's going good! I've worked out a number of bugs, and it seems to
be rendering right!

Now I believe all I need is to do is to change pages over to the new
text file system, and I should be good to go!

But first, I need to check what bugs I had back-logged.

There are also two other issues:
* the glossary
* the news page

Each of these may require special treatment.

The ''glossary'' may be a specially formatted page, with it's own sort
of conversion process.

Same with the ''news'' page, which needs to emit RSS and have entry
times somehow.


= 2005-06-04 =

== 6:23 PM ==

Been working on this for a while today. Got distracted by web (damn the
web!) and working on the MachineCodeBlocks today. But other than that,
it's been good today.

I'm about to commit the code, because I'm making a risky change: In
lnlink, I'm adding a class to assist in the names-collection and linking
process. It'll help keep track of a list of names to be linked, to link
them, to make sure you don't double-link (or allow you to re-link,)
something that helps you see what you've forgotten to link, and then
something that will link things that aren't linked yet to a
default.

That kind of stuff.

I think it'll help simplify the builder, and make things more friendly
all around.

Damn, I wish PEAK were well documented and mainstream. I see so many
ways to use it.


= 2005-06-03 =

== 6:32 PM ==

Okay, I think I've fixed the bug, I just need a test situation...

== 8:38 AM ==

Very briefly: I did a lot of work on it this morning, I got the
WebsiteBuilder.compile_names_dictionary working.

It's going well.

I have a bug to fix:

* lnlink is treating http://... as if it were a Local Name.


= 2005-06-02 =

== 12:50 AM ==

Shoot, it's too late. Anyways:

I need to put the recommended terms & languages notes in here. Or
somewhere. Maybe in the base directory? Maybe in some "direction"
directory, or with all the standards specifications, or..?

== 9:45 AM ==

It's going pretty well, so far. I've got the filesystem all abstracted
away. Now I'm working on the "SiteBuilder" class, moving in the contents
from __name__=='__main__' into there.


== 8:50 AM ==

Reworking to use the filesystem abstraction. I think I'm going to have
it just read some of the autostart files, and store them in
variables. No need to revisit them.


= 2005-06-01 =

== 8:21 PM ==

Okay, I'm back from working on SinghText. I believe we have everything
we need from over there- we can get a list of names in a given page, we
can make <a name='foo'>, we can do all these different things.

Now I just need to figure out what I need to do up here.

* I believe I was supposed to apply some filesystem abstraction in
  render.py.
* I believe it was supposed to use optparse.

So, let's take these in the following order:

# Rework to use filesystem abstraction.
# Setup optparse use.
# Start making it work.
** Build internal names table.
*** From pre-existing list.
*** Provided by pages. ($name)
*** ...and then those that are looked up from the Internet.
** Perform merging and collections and linking and whatnot.

So, I guess whenever I next get to code, it's reworking to the
filesystem abstraction that's up.


= 2005-05-30 =

== 2:23 PM ==

Uploading everything in the SVN tree. Arranging files, folders, text, ...

Renaming and repositioning some stuff, it'll be broken for a bit.


= 2005-05-25 =

== 9:56 AM ==

kw: todo

I need to make it so that localnames.txt is copied to the final
directory..!

== 9:53 AM ==

I need to make it an official part of the query spec, that names can be
relative addresses.

If a name does not begin with a protocol, then it is a relative address.

I need to figure out the formal basis for relative addressing off of
URLs, though, if there even is one.

== ~9:30 AM ==

lnlink and singhtext are now in the onebigsoup tree.

I want this whole space into there, with time.

I've been thinking: I can use a dictionary to store the names
binding. If something isn't found in the dictionary, THEN defer to the
network.

Or, read the names binding from the file, using the parser.

At any rate, the basic idea should work.


= 2005-05-25 =

PROBLEM:

* You have a program that makes use of several web pages, located
  all around the Internet.

* You run this program several times in a row. You also make use of
  other programs that may make use of web pages around the Internet,
  visiting the same (but also some different) pages.

* You want to cache your pulls- you don't want to keep pulling in
  the data, over and over and over again.

WHAT DO YOU DO,
  WHAT ''DO'' YOU DO?!?

SUPERPROBLEM:
 * I'm trying to resolve localnames from a text file that's not on the
   Internet.


possible solutions:
* Manually localnames.txt to the Internet before using it.
* Automaticall post localnames.txt, somehow, to the Internet, before
  using it.
* Post localnames.txt using the LN Store interface, before using it.
* Don't include localnames.txt as part of the website generation code.
* Write a resolver that works with local files, and can also defer to a
  remote names server.
* Extend the query interface to handle "here's a namespace's text" as
  part of a query.

I think I'll go with the namespace store.

